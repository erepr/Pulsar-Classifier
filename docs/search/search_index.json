{"config":{"lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"RESTRICTED DISTRIBUTION The information is standard Company Confidential, but due to its sensitivity, it has restricted distribution and viewing within iNeuron. Project Name: Pulsar Classifier Date Issued Version Description Author 17 August 2020 1.0 Initial Draft Anoop Parasar & Akshay Rahate Contributors Name Section Worked Upon Anoop Parasar Exploratory Data Analysis Akshay Rahate Model Building & Deployment Document Classification Classification Company Confidential Definition Information is Group confidential and needs to be protected Context Where the loss of information confidentiality would result in significant harm to the interests of the organisation, financial loss, embarrassment or loss of information","title":"Home"},{"location":"#restricted-distribution","text":"The information is standard Company Confidential, but due to its sensitivity, it has restricted distribution and viewing within iNeuron.","title":"RESTRICTED DISTRIBUTION"},{"location":"#project-name-pulsar-classifier","text":"Date Issued Version Description Author 17 August 2020 1.0 Initial Draft Anoop Parasar & Akshay Rahate","title":"Project Name: Pulsar Classifier"},{"location":"#contributors","text":"Name Section Worked Upon Anoop Parasar Exploratory Data Analysis Akshay Rahate Model Building & Deployment","title":"Contributors"},{"location":"#document-classification","text":"Classification Company Confidential Definition Information is Group confidential and needs to be protected Context Where the loss of information confidentiality would result in significant harm to the interests of the organisation, financial loss, embarrassment or loss of information","title":"Document Classification"},{"location":"calibrate/","text":"Model Calibration Model Calibration This function takes the input of trained estimator and performs probability calibration with sigmoid or isotonic regression. The output prints a score grid that shows Accuracy, AUC, Recall, Precision, F1 and Kappa by fold (default = 10 Fold). The output of the original estimator and the calibrated estimator (created using this function) might not differ much. In order to see the calibration differences, use \u2018calibration\u2019 plot in plot_model to see the difference before and after. Calibrate Extra Trees Classifier # et, tuned_et, calibrated_et calibrated_et = calibrate_model ( et ) Output Calibrate Tuned Extra Trees Classifier # et, tuned_et, calibrated_et calibrated_tuned_et = calibrate_model ( tuned_et ) Output Calibrate Random Forest Classifier # rf, tuned_rf, calibrated_rf calibrated_rf = calibrate_model ( rf ) Output Calibrate Tuned Random Forest Classifier # rf, tuned_rf, calibrated_rf calibrated_tuned_rf = calibrate_model ( tuned_rf ) Output","title":"Model Calibration"},{"location":"calibrate/#model-calibration","text":"Model Calibration This function takes the input of trained estimator and performs probability calibration with sigmoid or isotonic regression. The output prints a score grid that shows Accuracy, AUC, Recall, Precision, F1 and Kappa by fold (default = 10 Fold). The output of the original estimator and the calibrated estimator (created using this function) might not differ much. In order to see the calibration differences, use \u2018calibration\u2019 plot in plot_model to see the difference before and after.","title":"Model Calibration"},{"location":"calibrate/#calibrate-extra-trees-classifier","text":"# et, tuned_et, calibrated_et calibrated_et = calibrate_model ( et ) Output","title":"Calibrate Extra Trees Classifier"},{"location":"calibrate/#calibrate-tuned-extra-trees-classifier","text":"# et, tuned_et, calibrated_et calibrated_tuned_et = calibrate_model ( tuned_et ) Output","title":"Calibrate Tuned Extra Trees Classifier"},{"location":"calibrate/#calibrate-random-forest-classifier","text":"# rf, tuned_rf, calibrated_rf calibrated_rf = calibrate_model ( rf ) Output","title":"Calibrate Random Forest Classifier"},{"location":"calibrate/#calibrate-tuned-random-forest-classifier","text":"# rf, tuned_rf, calibrated_rf calibrated_tuned_rf = calibrate_model ( tuned_rf ) Output","title":"Calibrate Tuned Random Forest Classifier"},{"location":"conclusion/","text":"Findings & Conclusion Overall it was relatively good experience Data handling and EDA learnings How to handle null values How to apply basic stats to identify the data distribution Analyze the data in term of distribution Identify the number of classes in data set Identify the count of different classes in the data set Analyze the correlation with all variables with target variable and other variables Data handling through visualization learnings Learn about basic visualization tool like pair plot, correlation map, Box plot and Violin plot and identify the output based on these graphs Data preprocessing techniques Learn about basic data preprocessing techniques. Learn about how we handle unbalanced data sets and what are the main techniques to handle unbalanced data sets Model creation and model tuning learnings Generate the base model and define the hyper parameters Learn how to tune hyper-parameters Apply all main classification models on the data sets Evaluate each model in term of accuracy Model selection criteria Learn about how to compare different models results Checking the score of each model based on cross validation Visualize the model accuracy based on palette tool Check the AUC, ROC curve and different classification report like precision , recall, F1-score etc. Save the model in appropriate format Model format Learn about what is a difference between pickle format and ONNX format. Why ONNX format is used in the industry. Deployment learning How to deploy a model on Local environment How to deploy a model using Docker How to deploy a model on cloud (AWS) How to deploy model on AWS Elastic Kubernetes Service Learn about how cloud works and what are the main components require for cloud deployment User Interface learning How to build a User Web Interface to collect the required variables and display the output on web How to build a user web interface to collect the CSV file and display the output on web against bulk upload","title":"Findings & Conclusion"},{"location":"conclusion/#findings-conclusion","text":"Overall it was relatively good experience Data handling and EDA learnings How to handle null values How to apply basic stats to identify the data distribution Analyze the data in term of distribution Identify the number of classes in data set Identify the count of different classes in the data set Analyze the correlation with all variables with target variable and other variables Data handling through visualization learnings Learn about basic visualization tool like pair plot, correlation map, Box plot and Violin plot and identify the output based on these graphs Data preprocessing techniques Learn about basic data preprocessing techniques. Learn about how we handle unbalanced data sets and what are the main techniques to handle unbalanced data sets Model creation and model tuning learnings Generate the base model and define the hyper parameters Learn how to tune hyper-parameters Apply all main classification models on the data sets Evaluate each model in term of accuracy Model selection criteria Learn about how to compare different models results Checking the score of each model based on cross validation Visualize the model accuracy based on palette tool Check the AUC, ROC curve and different classification report like precision , recall, F1-score etc. Save the model in appropriate format Model format Learn about what is a difference between pickle format and ONNX format. Why ONNX format is used in the industry. Deployment learning How to deploy a model on Local environment How to deploy a model using Docker How to deploy a model on cloud (AWS) How to deploy model on AWS Elastic Kubernetes Service Learn about how cloud works and what are the main components require for cloud deployment User Interface learning How to build a User Web Interface to collect the required variables and display the output on web How to build a user web interface to collect the CSV file and display the output on web against bulk upload","title":"Findings &amp; Conclusion"},{"location":"create/","text":"Model Creation Create Extra Trees Classifier Model et = create_model ( 'et' ) Output Create Random Forest Classifier Model rf = create_model ( 'rf' ) Output","title":"Model Creation"},{"location":"create/#model-creation","text":"","title":"Model Creation"},{"location":"create/#create-extra-trees-classifier-model","text":"et = create_model ( 'et' ) Output","title":"Create Extra Trees Classifier Model"},{"location":"create/#create-random-forest-classifier-model","text":"rf = create_model ( 'rf' ) Output","title":"Create Random Forest Classifier Model"},{"location":"docker/","text":"Deploy your model locally Navigate to project folder cd pulsar-classifier Installs the tree utility apt-get update && apt-get install -y tree Create a Dockerfile Create a Dockerfile to build your custom image: mkdir my-api && cd my-api && touch Dockerfile sudo nano Dockerfile To save the changes you\u2019ve made to the file, press Ctrl+o . If the file doesn\u2019t already exist, it will be created once you save it. To exit nano press Ctrl+x . If there are unsaved changes, you\u2019ll be asked whether you want to save the changes. For Pickle Format # Dockerfile FROM cortexlabs/python-predictor-cpu-slim:0.18.1 RUN apt-get update \\ && apt-get install -y tree \\ && apt-get clean && rm -rf /var/lib/apt/lists/* RUN pip install --no-cache-dir pandas \\ && conda install -y conda-forge::rdkit \\ && conda clean -a For ONNX Format # Dockerfile FROM cortexlabs/onnx-predictor-cpu-slim:0.18.1 RUN apt-get update \\ && apt-get install -y tree \\ && apt-get clean && rm -rf /var/lib/apt/lists/* RUN pip install --no-cache-dir pandas \\ && conda install -y conda-forge::rdkit \\ && conda clean -a Build and push to a AWS Elastic Container Registry This creates ECR in us-east-2 region aws ecr create-repository --repository-name=pulsar-classifier --region=us-east-2 Warning Take note of repository url Build the image based on your Dockerfile and push it to its repository in ECR: Change <repository_url> with your ECR URI docker build . -t pulsar-classifier:latest -t <repository_url>:latest Log Into AWS ECR Docker Run below command and copy Account ID aws sts get-caller-identity Replace <Account ID> with your Account ID aws ecr get-login-password --region us-east-2 | docker login --username AWS --password-stdin <Account ID>.dkr.ecr.us-east-2.amazonaws.com Push docker image to its repository in ECR docker push <repository_url>:latest Configure Cortex API for Pickle Format Update your API configuration file to point to your image: Change <repository_url> with your ECR URI # cortex.yaml - name: pulsar-classifier predictor: type: python path: predictor.py config: bucket: pulsar key: pulsar-classifier/model.pkl image: <repository_url>:latest monitoring: model_type: classification Configure Cortex API for ONNX Format Update your API configuration file to point to your image: Change <repository_url> with your ECR URI cortex.yaml - name: pulsar-classifier predictor: type: onnx path: predictor.py model_path: s3//pulsar/pulsar-classifier/model.onnx image: <repository_url>:latest monitoring: model_type: classification Deploy your API as usual cortex deploy Verify Docker Running docker ps -a","title":"Dockerize Locally"},{"location":"docker/#deploy-your-model-locally","text":"Navigate to project folder cd pulsar-classifier Installs the tree utility apt-get update && apt-get install -y tree","title":"Deploy your model locally"},{"location":"docker/#create-a-dockerfile","text":"Create a Dockerfile to build your custom image: mkdir my-api && cd my-api && touch Dockerfile sudo nano Dockerfile To save the changes you\u2019ve made to the file, press Ctrl+o . If the file doesn\u2019t already exist, it will be created once you save it. To exit nano press Ctrl+x . If there are unsaved changes, you\u2019ll be asked whether you want to save the changes.","title":"Create a Dockerfile"},{"location":"docker/#for-pickle-format","text":"# Dockerfile FROM cortexlabs/python-predictor-cpu-slim:0.18.1 RUN apt-get update \\ && apt-get install -y tree \\ && apt-get clean && rm -rf /var/lib/apt/lists/* RUN pip install --no-cache-dir pandas \\ && conda install -y conda-forge::rdkit \\ && conda clean -a","title":"For Pickle Format"},{"location":"docker/#for-onnx-format","text":"# Dockerfile FROM cortexlabs/onnx-predictor-cpu-slim:0.18.1 RUN apt-get update \\ && apt-get install -y tree \\ && apt-get clean && rm -rf /var/lib/apt/lists/* RUN pip install --no-cache-dir pandas \\ && conda install -y conda-forge::rdkit \\ && conda clean -a","title":"For ONNX Format"},{"location":"docker/#build-and-push-to-a-aws-elastic-container-registry","text":"This creates ECR in us-east-2 region aws ecr create-repository --repository-name=pulsar-classifier --region=us-east-2 Warning Take note of repository url Build the image based on your Dockerfile and push it to its repository in ECR: Change <repository_url> with your ECR URI docker build . -t pulsar-classifier:latest -t <repository_url>:latest","title":"Build and push to a AWS Elastic Container Registry"},{"location":"docker/#log-into-aws-ecr-docker","text":"Run below command and copy Account ID aws sts get-caller-identity Replace <Account ID> with your Account ID aws ecr get-login-password --region us-east-2 | docker login --username AWS --password-stdin <Account ID>.dkr.ecr.us-east-2.amazonaws.com Push docker image to its repository in ECR docker push <repository_url>:latest","title":"Log Into AWS ECR Docker"},{"location":"docker/#configure-cortex-api-for-pickle-format","text":"Update your API configuration file to point to your image: Change <repository_url> with your ECR URI # cortex.yaml - name: pulsar-classifier predictor: type: python path: predictor.py config: bucket: pulsar key: pulsar-classifier/model.pkl image: <repository_url>:latest monitoring: model_type: classification","title":"Configure Cortex API for Pickle Format"},{"location":"docker/#configure-cortex-api-for-onnx-format","text":"Update your API configuration file to point to your image: Change <repository_url> with your ECR URI cortex.yaml - name: pulsar-classifier predictor: type: onnx path: predictor.py model_path: s3//pulsar/pulsar-classifier/model.onnx image: <repository_url>:latest monitoring: model_type: classification","title":"Configure Cortex API for ONNX Format"},{"location":"docker/#deploy-your-api-as-usual","text":"cortex deploy","title":"Deploy your API as usual"},{"location":"docker/#verify-docker-running","text":"docker ps -a","title":"Verify Docker Running"},{"location":"eda/","text":"Exploratory Data Analysis Import Matplotlib & Seaborn import matplotlib.pyplot as plt # basic plotting library import seaborn as sns Unique Values print ( \"mean_int_pf: \" , len ( df [ 'mean_int_pf' ] . unique ())) print ( \"std_pf: \" , len ( df [ 'std_pf' ] . unique ())) print ( \"ex_kurt_pf: \" , len ( df [ 'ex_kurt_pf' ] . unique ())) print ( \"skew_pf: \" , len ( df [ 'skew_pf' ] . unique ())) print ( \"mean_dm: \" , len ( df [ 'mean_dm' ] . unique ())) print ( \"std_dm: \" , len ( df [ 'std_dm' ] . unique ())) print ( \"kurt_dm: \" , len ( df [ 'kurt_dm' ] . unique ())) print ( \"skew_dm: \" , len ( df [ 'skew_dm' ] . unique ())) Output mean_int_pf: 8626 std_pf: 17862 ex_kurt_pf: 17897 skew_pf: 17898 mean_dm: 9000 std_dm: 17894 kurt_dm: 17895 skew_dm: 17895 Missing Data Check This will display the detailed information, like there is no missing data in the data set df . info () Output RangeIndex: 17898 entries, 1 to 17898 Data columns (total 9 columns): mean_int_pf 17898 non-null float64 std_pf 17898 non-null float64 ex_kurt_pf 17898 non-null float64 skew_pf 17898 non-null float64 mean_dm 17898 non-null float64 std_dm 17898 non-null float64 kurt_dm 17898 non-null float64 skew_dm 17898 non-null float64 class 17898 non-null int64 dtypes: float64(8), int64(1) memory usage: 1.2 MB Check Imbalanced Dataset It shows data is unbalanced df [ 'class' ] . value_counts () Output 0 16259 1 1639 Name: class, dtype: int64 count_classes = pd . value_counts ( df [ 'class' ], sort = True ) count_classes . plot ( kind = 'bar' , rot = 0 ) plt . title ( \"Transaction Class Distribution\" ) #plt.xticks(range(2), LABELS) plt . xlabel ( \"Class\" ) plt . ylabel ( \"Frequency\" ) Output Pairplot sns . pairplot ( data = df , palette = \"husl\" , hue = \"class\" , vars = [ \"mean_int_pf\" , \"std_pf\" , \"ex_kurt_pf\" , \"skew_pf\" , \"mean_dm\" , \"std_dm\" , \"kurt_dm\" , \"skew_dm\" ]) plt . suptitle ( \" \\n PairPlot of df \\n \\n \" , fontsize = 18 ) plt . tight_layout () plt . show () # pairplot without standard deviaton fields of data Data shows its a good correlation between following fields. There are good correlation between ex_kurt_pf and skew_pf and there is a significant correlation between ex_kurt_pf and class. There is good correlation between mean_dm and std_dm There are good correlation between std_dm and mean_dm and inverse correlation between std_dm and kurt_dm There is good correlation between kurt_dm and skew_dm There is good correlation between skew_dm and kurt_dm Our target variable has good correlation with ex_kurt_pf and skew_pf variables Correlation HeatMap plt . figure ( figsize = ( 12 , 10 )) #sns.heatmap(data=df.corr(),annot=True,cmap=\"bone\",linewidths=1,fmt=\".2f\",linecolor=\"gray\") #plt.figure(figsize=(17,8)) cbar_kws = { 'ticks' : [ - 1 , - 0.5 , 0 , 0.5 , 1 ], 'orientation' : 'vertical' } sns . heatmap ( df . corr (), cbar_kws = cbar_kws , cmap = 'OrRd' , annot = True , linewidths = 0.3 ) plt . xticks ( rotation = 0 ) plt . title ( \"Correlation Map\" , fontsize = 20 ) plt . tight_layout () plt . show () Lightest and darkest cells are most correlated ones Most of our Columns are already related or derived from one or another. And we can see it clearly on some Cells above Check Outliers plt . subplot ( 2 , 2 , 1 ) sns . boxplot ( data = df , y = \"mean_int_pf\" , x = \"class\" ) plt . subplot ( 2 , 2 , 2 ) sns . boxplot ( data = df , y = \"std_pf\" , x = \"class\" ) plt . subplot ( 2 , 2 , 3 ) sns . boxplot ( data = df , y = \"ex_kurt_pf\" , x = \"class\" ) plt . subplot ( 2 , 2 , 4 ) sns . boxplot ( data = df , y = \"skew_pf\" , x = \"class\" ) plt . suptitle ( \"BoxPlot\" , fontsize = 20 ) plt . show () plt . subplot ( 2 , 2 , 1 ) sns . boxplot ( data = df , y = \"mean_dm\" , x = \"class\" ) plt . subplot ( 2 , 2 , 2 ) sns . boxplot ( data = df , y = \"std_dm\" , x = \"class\" ) plt . subplot ( 2 , 2 , 3 ) sns . boxplot ( data = df , y = \"kurt_dm\" , x = \"class\" ) plt . subplot ( 2 , 2 , 4 ) sns . boxplot ( data = df , y = \"skew_dm\" , x = \"class\" ) plt . suptitle ( \"BoxPlot\" , fontsize = 20 ) plt . show () Histogram Distribution f , axes = plt . subplots ( 2 , 4 , figsize = ( 15 , 8 )) sns . distplot ( df [ \"mean_int_pf\" ], ax = axes [ 0 , 0 ]) sns . distplot ( df [ \"std_pf\" ], ax = axes [ 0 , 1 ]) sns . distplot ( df [ \"ex_kurt_pf\" ], ax = axes [ 0 , 2 ]) sns . distplot ( df [ \"skew_pf\" ], ax = axes [ 0 , 3 ]) sns . distplot ( df [ \"mean_dm\" ], ax = axes [ 1 , 0 ]) sns . distplot ( df [ \"std_dm\" ], ax = axes [ 1 , 1 ]) sns . distplot ( df [ \"kurt_dm\" ], ax = axes [ 1 , 2 ]) sns . distplot ( df [ \"skew_dm\" ], ax = axes [ 1 , 3 ]) Above graph representation shows that data is left skewed in following parameters. mean_dm, std_dm, skew_pf, ek_kurt_pf, skew_dm, std_dm Data of following columns are normally distributed mean_df and kurt_dm","title":"Exploratory Data Analysis"},{"location":"eda/#exploratory-data-analysis","text":"","title":"Exploratory Data Analysis"},{"location":"eda/#import-matplotlib-seaborn","text":"import matplotlib.pyplot as plt # basic plotting library import seaborn as sns","title":"Import Matplotlib &amp; Seaborn"},{"location":"eda/#unique-values","text":"print ( \"mean_int_pf: \" , len ( df [ 'mean_int_pf' ] . unique ())) print ( \"std_pf: \" , len ( df [ 'std_pf' ] . unique ())) print ( \"ex_kurt_pf: \" , len ( df [ 'ex_kurt_pf' ] . unique ())) print ( \"skew_pf: \" , len ( df [ 'skew_pf' ] . unique ())) print ( \"mean_dm: \" , len ( df [ 'mean_dm' ] . unique ())) print ( \"std_dm: \" , len ( df [ 'std_dm' ] . unique ())) print ( \"kurt_dm: \" , len ( df [ 'kurt_dm' ] . unique ())) print ( \"skew_dm: \" , len ( df [ 'skew_dm' ] . unique ())) Output mean_int_pf: 8626 std_pf: 17862 ex_kurt_pf: 17897 skew_pf: 17898 mean_dm: 9000 std_dm: 17894 kurt_dm: 17895 skew_dm: 17895","title":"Unique Values"},{"location":"eda/#missing-data-check","text":"This will display the detailed information, like there is no missing data in the data set df . info () Output RangeIndex: 17898 entries, 1 to 17898 Data columns (total 9 columns): mean_int_pf 17898 non-null float64 std_pf 17898 non-null float64 ex_kurt_pf 17898 non-null float64 skew_pf 17898 non-null float64 mean_dm 17898 non-null float64 std_dm 17898 non-null float64 kurt_dm 17898 non-null float64 skew_dm 17898 non-null float64 class 17898 non-null int64 dtypes: float64(8), int64(1) memory usage: 1.2 MB","title":"Missing Data Check"},{"location":"eda/#check-imbalanced-dataset","text":"It shows data is unbalanced df [ 'class' ] . value_counts () Output 0 16259 1 1639 Name: class, dtype: int64 count_classes = pd . value_counts ( df [ 'class' ], sort = True ) count_classes . plot ( kind = 'bar' , rot = 0 ) plt . title ( \"Transaction Class Distribution\" ) #plt.xticks(range(2), LABELS) plt . xlabel ( \"Class\" ) plt . ylabel ( \"Frequency\" ) Output","title":"Check Imbalanced Dataset"},{"location":"eda/#pairplot","text":"sns . pairplot ( data = df , palette = \"husl\" , hue = \"class\" , vars = [ \"mean_int_pf\" , \"std_pf\" , \"ex_kurt_pf\" , \"skew_pf\" , \"mean_dm\" , \"std_dm\" , \"kurt_dm\" , \"skew_dm\" ]) plt . suptitle ( \" \\n PairPlot of df \\n \\n \" , fontsize = 18 ) plt . tight_layout () plt . show () # pairplot without standard deviaton fields of data Data shows its a good correlation between following fields. There are good correlation between ex_kurt_pf and skew_pf and there is a significant correlation between ex_kurt_pf and class. There is good correlation between mean_dm and std_dm There are good correlation between std_dm and mean_dm and inverse correlation between std_dm and kurt_dm There is good correlation between kurt_dm and skew_dm There is good correlation between skew_dm and kurt_dm Our target variable has good correlation with ex_kurt_pf and skew_pf variables","title":"Pairplot"},{"location":"eda/#correlation-heatmap","text":"plt . figure ( figsize = ( 12 , 10 )) #sns.heatmap(data=df.corr(),annot=True,cmap=\"bone\",linewidths=1,fmt=\".2f\",linecolor=\"gray\") #plt.figure(figsize=(17,8)) cbar_kws = { 'ticks' : [ - 1 , - 0.5 , 0 , 0.5 , 1 ], 'orientation' : 'vertical' } sns . heatmap ( df . corr (), cbar_kws = cbar_kws , cmap = 'OrRd' , annot = True , linewidths = 0.3 ) plt . xticks ( rotation = 0 ) plt . title ( \"Correlation Map\" , fontsize = 20 ) plt . tight_layout () plt . show () Lightest and darkest cells are most correlated ones Most of our Columns are already related or derived from one or another. And we can see it clearly on some Cells above","title":"Correlation HeatMap"},{"location":"eda/#check-outliers","text":"plt . subplot ( 2 , 2 , 1 ) sns . boxplot ( data = df , y = \"mean_int_pf\" , x = \"class\" ) plt . subplot ( 2 , 2 , 2 ) sns . boxplot ( data = df , y = \"std_pf\" , x = \"class\" ) plt . subplot ( 2 , 2 , 3 ) sns . boxplot ( data = df , y = \"ex_kurt_pf\" , x = \"class\" ) plt . subplot ( 2 , 2 , 4 ) sns . boxplot ( data = df , y = \"skew_pf\" , x = \"class\" ) plt . suptitle ( \"BoxPlot\" , fontsize = 20 ) plt . show () plt . subplot ( 2 , 2 , 1 ) sns . boxplot ( data = df , y = \"mean_dm\" , x = \"class\" ) plt . subplot ( 2 , 2 , 2 ) sns . boxplot ( data = df , y = \"std_dm\" , x = \"class\" ) plt . subplot ( 2 , 2 , 3 ) sns . boxplot ( data = df , y = \"kurt_dm\" , x = \"class\" ) plt . subplot ( 2 , 2 , 4 ) sns . boxplot ( data = df , y = \"skew_dm\" , x = \"class\" ) plt . suptitle ( \"BoxPlot\" , fontsize = 20 ) plt . show ()","title":"Check Outliers"},{"location":"eda/#histogram-distribution","text":"f , axes = plt . subplots ( 2 , 4 , figsize = ( 15 , 8 )) sns . distplot ( df [ \"mean_int_pf\" ], ax = axes [ 0 , 0 ]) sns . distplot ( df [ \"std_pf\" ], ax = axes [ 0 , 1 ]) sns . distplot ( df [ \"ex_kurt_pf\" ], ax = axes [ 0 , 2 ]) sns . distplot ( df [ \"skew_pf\" ], ax = axes [ 0 , 3 ]) sns . distplot ( df [ \"mean_dm\" ], ax = axes [ 1 , 0 ]) sns . distplot ( df [ \"std_dm\" ], ax = axes [ 1 , 1 ]) sns . distplot ( df [ \"kurt_dm\" ], ax = axes [ 1 , 2 ]) sns . distplot ( df [ \"skew_dm\" ], ax = axes [ 1 , 3 ]) Above graph representation shows that data is left skewed in following parameters. mean_dm, std_dm, skew_pf, ek_kurt_pf, skew_dm, std_dm Data of following columns are normally distributed mean_df and kurt_dm","title":"Histogram Distribution"},{"location":"eks/","text":"AWS EKS AWS EKS Deployment This may incur charge This may incur charge as it creates m3.large cluster which is outside free tier scheme. Creating Project files ./pulsar-classifier/ \u251c\u2500\u2500 cortex.yaml \u251c\u2500\u2500 predictor.py \u2514\u2500\u2500 requirements.txt \u2514\u2500\u2500 cluster.yaml Cluster configuration Create a cluster.yaml file with exact configuration below. Just replace aws_access_key_id and aws_secret_access_key with your credentials. This would create cluster with following configuration. AWS region us-east-2 An S3 Bucket for metadata storage m3.large EC2 instance SSD Disk Storage 30 GB CloudWatch log group name pulsar # cluster.yaml # AWS credentials (if not specified, ~/.aws/credentials will be checked) (can be overridden by $AWS_ACCESS_KEY_ID and $AWS_SECRET_ACCESS_KEY) aws_access_key_id: *** aws_secret_access_key: *** # optional AWS credentials for the operator which may be used to restrict its AWS access (defaults to the AWS credentials set above) cortex_aws_access_key_id: *** cortex_aws_secret_access_key: *** # EKS cluster name for cortex (default: cortex) cluster_name: pulsar # AWS region region: us-east-2 # S3 bucket (default: <cluster_name>-<RANDOM_ID>) # note: your cortex cluster uses this bucket for metadata storage, and it should not be accessed directly (a separate bucket should be used for your models) bucket: # cortex-<RANDOM_ID> # list of availability zones for your region (default: 3 random availability zones from the specified region) availability_zones: # e.g. [us-east-1a, us-east-1b, us-east-1c] # instance type instance_type: m3.large # minimum number of instances (must be >= 0) min_instances: 1 # maximum number of instances (must be >= 1) max_instances: 1 # disk storage size per instance (GB) (default: 50) instance_volume_size: 30 # instance volume type [gp2, io1, st1, sc1] (default: gp2) instance_volume_type: gp2 # instance volume iops (only applicable to io1 storage type) (default: 3000) # instance_volume_iops: 3000 # whether the subnets used for EC2 instances should be public or private (default: \"public\") # if \"public\", instances will be assigned public IP addresses; if \"private\", instances won't have public IPs and a NAT gateway will be created to allow outgoing network requests # see https://docs.cortex.dev/v/0.18/miscellaneous/security#private-cluster for more information subnet_visibility: public # must be \"public\" or \"private\" # whether to include a NAT gateway with the cluster (a NAT gateway is necessary when using private subnets) # default value is \"none\" if subnet_visibility is set to \"public\"; \"single\" if subnet_visibility is \"private\" nat_gateway: none # must be \"none\", \"single\", or \"highly_available\" (highly_available means one NAT gateway per availability zone) # whether the API load balancer should be internet-facing or internal (default: \"internet-facing\") # note: if using \"internal\", APIs will still be accessible via the public API Gateway endpoint unless you also disable API Gateway in your API's configuration (if you do that, you must configure VPC Peering to connect to your APIs) # see https://docs.cortex.dev/v/0.18/miscellaneous/security#private-cluster for more information api_load_balancer_scheme: internet-facing # must be \"internet-facing\" or \"internal\" # whether the operator load balancer should be internet-facing or internal (default: \"internet-facing\") # note: if using \"internal\", you must configure VPC Peering to connect your CLI to your cluster operator (https://docs.cortex.dev/v/0.18/guides/vpc-peering) # see https://docs.cortex.dev/v/0.18/miscellaneous/security#private-cluster for more information operator_load_balancer_scheme: internet-facing # must be \"internet-facing\" or \"internal\" # CloudWatch log group for cortex (default: <cluster_name>) log_group: pulsar # additional tags to assign to aws resources for labelling and cost allocation (by default, all resources will be tagged with cortex.dev/cluster-name=<cluster_name>) tags: # <string>: <string> map of key/value pairs # whether to use spot instances in the cluster (default: false) # see https://docs.cortex.dev/v/0.18/cluster-management/spot-instances for additional details on spot configuration spot: false # see https://docs.cortex.dev/v/0.18/guides/custom-domain for instructions on how to set up a custom domain ssl_certificate_arn: Create Cluster This creates a Cortex cluster in your AWS account, and will take approximately 15 minutes. cortex cluster up --config cluster.yaml Deploy Cluster cortex deploy --env aws Get API's endpoint cortex get pulsar-classifier --env aws Configure prediction monitoring # cortex.yaml - name: pulsar-classifier predictor: type: python path: predictor.py config: bucket: pulsar key: pulsar-classifier/model.pkl monitoring: model_type: classification Watch API Past Prediction cortex get --env aws pulsar-classifier --watch AWS CloudWatch Monitoring Visit your CloudWatch resource and click on pulsar Spin down a cluster cortex cluster down Delete Cluster Environment cortex env delete Delete Cluster from AWS cortex delete pulsar-classifier --env aws","title":"AWS EKS"},{"location":"eks/#aws-eks","text":"","title":"AWS EKS"},{"location":"eks/#aws-eks-deployment","text":"This may incur charge This may incur charge as it creates m3.large cluster which is outside free tier scheme.","title":"AWS EKS Deployment"},{"location":"eks/#creating-project-files","text":"./pulsar-classifier/ \u251c\u2500\u2500 cortex.yaml \u251c\u2500\u2500 predictor.py \u2514\u2500\u2500 requirements.txt \u2514\u2500\u2500 cluster.yaml","title":"Creating Project files"},{"location":"eks/#cluster-configuration","text":"Create a cluster.yaml file with exact configuration below. Just replace aws_access_key_id and aws_secret_access_key with your credentials. This would create cluster with following configuration. AWS region us-east-2 An S3 Bucket for metadata storage m3.large EC2 instance SSD Disk Storage 30 GB CloudWatch log group name pulsar # cluster.yaml # AWS credentials (if not specified, ~/.aws/credentials will be checked) (can be overridden by $AWS_ACCESS_KEY_ID and $AWS_SECRET_ACCESS_KEY) aws_access_key_id: *** aws_secret_access_key: *** # optional AWS credentials for the operator which may be used to restrict its AWS access (defaults to the AWS credentials set above) cortex_aws_access_key_id: *** cortex_aws_secret_access_key: *** # EKS cluster name for cortex (default: cortex) cluster_name: pulsar # AWS region region: us-east-2 # S3 bucket (default: <cluster_name>-<RANDOM_ID>) # note: your cortex cluster uses this bucket for metadata storage, and it should not be accessed directly (a separate bucket should be used for your models) bucket: # cortex-<RANDOM_ID> # list of availability zones for your region (default: 3 random availability zones from the specified region) availability_zones: # e.g. [us-east-1a, us-east-1b, us-east-1c] # instance type instance_type: m3.large # minimum number of instances (must be >= 0) min_instances: 1 # maximum number of instances (must be >= 1) max_instances: 1 # disk storage size per instance (GB) (default: 50) instance_volume_size: 30 # instance volume type [gp2, io1, st1, sc1] (default: gp2) instance_volume_type: gp2 # instance volume iops (only applicable to io1 storage type) (default: 3000) # instance_volume_iops: 3000 # whether the subnets used for EC2 instances should be public or private (default: \"public\") # if \"public\", instances will be assigned public IP addresses; if \"private\", instances won't have public IPs and a NAT gateway will be created to allow outgoing network requests # see https://docs.cortex.dev/v/0.18/miscellaneous/security#private-cluster for more information subnet_visibility: public # must be \"public\" or \"private\" # whether to include a NAT gateway with the cluster (a NAT gateway is necessary when using private subnets) # default value is \"none\" if subnet_visibility is set to \"public\"; \"single\" if subnet_visibility is \"private\" nat_gateway: none # must be \"none\", \"single\", or \"highly_available\" (highly_available means one NAT gateway per availability zone) # whether the API load balancer should be internet-facing or internal (default: \"internet-facing\") # note: if using \"internal\", APIs will still be accessible via the public API Gateway endpoint unless you also disable API Gateway in your API's configuration (if you do that, you must configure VPC Peering to connect to your APIs) # see https://docs.cortex.dev/v/0.18/miscellaneous/security#private-cluster for more information api_load_balancer_scheme: internet-facing # must be \"internet-facing\" or \"internal\" # whether the operator load balancer should be internet-facing or internal (default: \"internet-facing\") # note: if using \"internal\", you must configure VPC Peering to connect your CLI to your cluster operator (https://docs.cortex.dev/v/0.18/guides/vpc-peering) # see https://docs.cortex.dev/v/0.18/miscellaneous/security#private-cluster for more information operator_load_balancer_scheme: internet-facing # must be \"internet-facing\" or \"internal\" # CloudWatch log group for cortex (default: <cluster_name>) log_group: pulsar # additional tags to assign to aws resources for labelling and cost allocation (by default, all resources will be tagged with cortex.dev/cluster-name=<cluster_name>) tags: # <string>: <string> map of key/value pairs # whether to use spot instances in the cluster (default: false) # see https://docs.cortex.dev/v/0.18/cluster-management/spot-instances for additional details on spot configuration spot: false # see https://docs.cortex.dev/v/0.18/guides/custom-domain for instructions on how to set up a custom domain ssl_certificate_arn:","title":"Cluster configuration"},{"location":"eks/#create-cluster","text":"This creates a Cortex cluster in your AWS account, and will take approximately 15 minutes. cortex cluster up --config cluster.yaml","title":"Create Cluster"},{"location":"eks/#deploy-cluster","text":"cortex deploy --env aws","title":"Deploy Cluster"},{"location":"eks/#get-apis-endpoint","text":"cortex get pulsar-classifier --env aws","title":"Get API's endpoint"},{"location":"eks/#configure-prediction-monitoring","text":"# cortex.yaml - name: pulsar-classifier predictor: type: python path: predictor.py config: bucket: pulsar key: pulsar-classifier/model.pkl monitoring: model_type: classification","title":"Configure prediction monitoring"},{"location":"eks/#watch-api-past-prediction","text":"cortex get --env aws pulsar-classifier --watch","title":"Watch API Past Prediction"},{"location":"eks/#aws-cloudwatch-monitoring","text":"Visit your CloudWatch resource and click on pulsar","title":"AWS CloudWatch Monitoring"},{"location":"eks/#spin-down-a-cluster","text":"cortex cluster down","title":"Spin down a cluster"},{"location":"eks/#delete-cluster-environment","text":"cortex env delete","title":"Delete Cluster Environment"},{"location":"eks/#delete-cluster-from-aws","text":"cortex delete pulsar-classifier --env aws","title":"Delete Cluster from AWS"},{"location":"final/","text":"Finalize Model Finalize Model We can see that Tuned Random Forest Model is giving better result. We mainly focus on Recall metrics because: Recall tells us what proportion of stars that actually are pulsar star but was predicted by the algorithm as pulsar star. The actual positives (Stars that are pulsar are TP and FN) and the stars predicted by the model being pulsars are TP. final_rf = finalize_model ( tuned_rf ) print ( final_rf ) RandomForestClassifier(bootstrap=False, ccp_alpha=0.0, class_weight=None, criterion='entropy', max_depth=90, max_features='auto', max_leaf_nodes=None, max_samples=None, min_impurity_decrease=0.0, min_impurity_split=None, min_samples_leaf=2, min_samples_split=10, min_weight_fraction_leaf=0.0, n_estimators=70, n_jobs=-1, oob_score=False, random_state=1007, verbose=0, warm_start=False)","title":"Finalize Model"},{"location":"final/#finalize-model","text":"Finalize Model We can see that Tuned Random Forest Model is giving better result. We mainly focus on Recall metrics because: Recall tells us what proportion of stars that actually are pulsar star but was predicted by the algorithm as pulsar star. The actual positives (Stars that are pulsar are TP and FN) and the stars predicted by the model being pulsars are TP. final_rf = finalize_model ( tuned_rf ) print ( final_rf ) RandomForestClassifier(bootstrap=False, ccp_alpha=0.0, class_weight=None, criterion='entropy', max_depth=90, max_features='auto', max_leaf_nodes=None, max_samples=None, min_impurity_decrease=0.0, min_impurity_split=None, min_samples_leaf=2, min_samples_split=10, min_weight_fraction_leaf=0.0, n_estimators=70, n_jobs=-1, oob_score=False, random_state=1007, verbose=0, warm_start=False)","title":"Finalize Model"},{"location":"first/","text":"First For full documentation visit mkdocs.org . Commands mkdocs new [dir-name] - Create a new project. mkdocs serve - Start the live-reloading docs server. mkdocs build - Build the documentation site. mkdocs -h - Print help message and exit. Project layout mkdocs.yml # The configuration file. docs/ index.md # The documentation homepage. ... # Other markdown pages, images and other files.","title":"First"},{"location":"first/#first","text":"For full documentation visit mkdocs.org .","title":"First"},{"location":"first/#commands","text":"mkdocs new [dir-name] - Create a new project. mkdocs serve - Start the live-reloading docs server. mkdocs build - Build the documentation site. mkdocs -h - Print help message and exit.","title":"Commands"},{"location":"first/#project-layout","text":"mkdocs.yml # The configuration file. docs/ index.md # The documentation homepage. ... # Other markdown pages, images and other files.","title":"Project layout"},{"location":"folder/","text":"Folder Structure Creating Project files For Pickle Format Once your model is exported, you can implement one of Cortex's Predictor classes to deploy your model. A Predictor is a Python class that describes how to initialize your model and use it to make predictions. Cortex makes all files in the project directory (i.e. the directory which contains cortex.yaml) available for use in your Predictor implementation. ./pulsar-classifier/ \u251c\u2500\u2500 cortex.yaml \u251c\u2500\u2500 predictor.py \u2514\u2500\u2500 requirements.txt Creating pulsar-classifier folder mkdir pulsar-classifier Python Predictor import os import boto3 from botocore import UNSIGNED from botocore.client import Config import pickle labels = [ 0 , 1 ] class PythonPredictor : def __init__ ( self , config ): if os . environ . get ( \"AWS_ACCESS_KEY_ID\" ): s3 = boto3 . client ( \"s3\" ) # client will use your credentials if available else : s3 = boto3 . client ( \"s3\" , config = Config ( signature_version = UNSIGNED )) # anonymous client s3 . download_file ( config [ \"bucket\" ], config [ \"key\" ], \"/tmp/model.pkl\" ) self . model = pickle . load ( open ( \"/tmp/model.pkl\" , \"rb\" )) def predict ( self , payload ): measurements = [ payload [ \"mean_int_pf\" ], payload [ \"std_pf\" ], payload [ \"ex_kurt_pf\" ], payload [ \"skew_pf\" ], payload [ \"mean_dm\" ], payload [ \"std_dm\" ], payload [ \"kurt_dm\" ], payload [ \"skew_dm\" ], ] label_id = self . model . predict ([ measurements ])[ 0 ][ 0 ] return labels [ label_id ] Specify your Python dependencies Create a requirements.txt file to specify the dependencies needed by predictor.py . Cortex will automatically install them into your runtime once you deploy: # requirements.txt boto3 Configure your API Create a cortex.yaml file and add the configuration below . An api provides a runtime for inference and makes your predictor.py implementation available as a web service that can serve real-time predictions: # cortex.yaml - name: pulsar-classifier predictor: type: python path: predictor.py config: bucket: pulsar key: pulsar-classifier/model.pkl monitoring: model_type: classification Creating Project files For ONNX Format Creating pulsar-classifier folder mkdir pulsar-classifier Folder Structure ./pulsar-classifier/ \u251c\u2500\u2500 cortex.yaml \u251c\u2500\u2500 predictor.py Python Predictor predictor.py labels = [ 0 , 1 ] class ONNXPredictor : def __init__ ( self , onnx_client , config ): self . client = onnx_client def predict ( self , payload ): model_input = [ payload [ \"mean_int_pf\" ], payload [ \"std_pf\" ], payload [ \"ex_kurt_pf\" ], payload [ \"skew_pf\" ], payload [ \"mean_dm\" ], payload [ \"std_dm\" ], payload [ \"kurt_dm\" ], payload [ \"skew_dm\" ], ] prediction = self . client . predict ( model_input ) predicted_class_id = prediction [ 0 ][ 0 ] return labels [ predicted_class_id ] Configure your API cortex.yaml - name: pulsar-classifier predictor: type: onnx path: predictor.py model_path: s3//pulsar/pulsar-classifier/model.onnx monitoring: model_type: classification","title":"Folder Structure"},{"location":"folder/#folder-structure","text":"","title":"Folder Structure"},{"location":"folder/#creating-project-files-for-pickle-format","text":"Once your model is exported, you can implement one of Cortex's Predictor classes to deploy your model. A Predictor is a Python class that describes how to initialize your model and use it to make predictions. Cortex makes all files in the project directory (i.e. the directory which contains cortex.yaml) available for use in your Predictor implementation. ./pulsar-classifier/ \u251c\u2500\u2500 cortex.yaml \u251c\u2500\u2500 predictor.py \u2514\u2500\u2500 requirements.txt Creating pulsar-classifier folder mkdir pulsar-classifier","title":"Creating Project files For Pickle Format"},{"location":"folder/#python-predictor","text":"import os import boto3 from botocore import UNSIGNED from botocore.client import Config import pickle labels = [ 0 , 1 ] class PythonPredictor : def __init__ ( self , config ): if os . environ . get ( \"AWS_ACCESS_KEY_ID\" ): s3 = boto3 . client ( \"s3\" ) # client will use your credentials if available else : s3 = boto3 . client ( \"s3\" , config = Config ( signature_version = UNSIGNED )) # anonymous client s3 . download_file ( config [ \"bucket\" ], config [ \"key\" ], \"/tmp/model.pkl\" ) self . model = pickle . load ( open ( \"/tmp/model.pkl\" , \"rb\" )) def predict ( self , payload ): measurements = [ payload [ \"mean_int_pf\" ], payload [ \"std_pf\" ], payload [ \"ex_kurt_pf\" ], payload [ \"skew_pf\" ], payload [ \"mean_dm\" ], payload [ \"std_dm\" ], payload [ \"kurt_dm\" ], payload [ \"skew_dm\" ], ] label_id = self . model . predict ([ measurements ])[ 0 ][ 0 ] return labels [ label_id ]","title":"Python Predictor"},{"location":"folder/#specify-your-python-dependencies","text":"Create a requirements.txt file to specify the dependencies needed by predictor.py . Cortex will automatically install them into your runtime once you deploy: # requirements.txt boto3","title":"Specify your Python dependencies"},{"location":"folder/#configure-your-api","text":"Create a cortex.yaml file and add the configuration below . An api provides a runtime for inference and makes your predictor.py implementation available as a web service that can serve real-time predictions: # cortex.yaml - name: pulsar-classifier predictor: type: python path: predictor.py config: bucket: pulsar key: pulsar-classifier/model.pkl monitoring: model_type: classification","title":"Configure your API"},{"location":"folder/#creating-project-files-for-onnx-format","text":"Creating pulsar-classifier folder mkdir pulsar-classifier","title":"Creating Project files For ONNX Format"},{"location":"folder/#folder-structure_1","text":"./pulsar-classifier/ \u251c\u2500\u2500 cortex.yaml \u251c\u2500\u2500 predictor.py","title":"Folder Structure"},{"location":"folder/#python-predictor_1","text":"predictor.py labels = [ 0 , 1 ] class ONNXPredictor : def __init__ ( self , onnx_client , config ): self . client = onnx_client def predict ( self , payload ): model_input = [ payload [ \"mean_int_pf\" ], payload [ \"std_pf\" ], payload [ \"ex_kurt_pf\" ], payload [ \"skew_pf\" ], payload [ \"mean_dm\" ], payload [ \"std_dm\" ], payload [ \"kurt_dm\" ], payload [ \"skew_dm\" ], ] prediction = self . client . predict ( model_input ) predicted_class_id = prediction [ 0 ][ 0 ] return labels [ predicted_class_id ]","title":"Python Predictor"},{"location":"folder/#configure-your-api_1","text":"cortex.yaml - name: pulsar-classifier predictor: type: onnx path: predictor.py model_path: s3//pulsar/pulsar-classifier/model.onnx monitoring: model_type: classification","title":"Configure your API"},{"location":"ingestion/","text":"Data Ingestion Data Ingestion Code import pandas as pd df = pd . read_csv ( \"HTRU_2.csv\" , header = None ) df . rename ( columns = { 0 : \"mean_int_pf\" , 1 : \"std_pf\" , 2 : \"ex_kurt_pf\" , 3 : \"skew_pf\" , 4 : \"mean_dm\" , 5 : \"std_dm\" , 6 : \"kurt_dm\" , 7 : \"skew_dm\" , 8 : \"class\" }, inplace = True ) df . index = df . index + 1 df . head ( 10 ) Output","title":"Data Ingestion"},{"location":"ingestion/#data-ingestion","text":"","title":"Data Ingestion"},{"location":"ingestion/#data-ingestion_1","text":"","title":"Data Ingestion"},{"location":"ingestion/#code","text":"import pandas as pd df = pd . read_csv ( \"HTRU_2.csv\" , header = None ) df . rename ( columns = { 0 : \"mean_int_pf\" , 1 : \"std_pf\" , 2 : \"ex_kurt_pf\" , 3 : \"skew_pf\" , 4 : \"mean_dm\" , 5 : \"std_dm\" , 6 : \"kurt_dm\" , 7 : \"skew_dm\" , 8 : \"class\" }, inplace = True ) df . index = df . index + 1 df . head ( 10 ) Output","title":"Code"},{"location":"introduction/","text":"Introduction Data Description Pulsar candidates collected during the HTRU survey. Pulsars are a type of star, of considerable scientific interest. Candidates must be classified in to pulsar and non-pulsar classes to aid discovery. For full documentation visit HTRU2 Data Set . The data set shared here contains 16,259 spurious examples caused by RFI/noise, and 1,639 real pulsar examples. These examples have all been checked by human annotators. So our goal is solving the classification problem and predict which stars are pulsar. The technical design document gives a design blueprint of the Pulsar Classifier project. This document communicates the technical details of the solution proposed. In addition, this document also captures the different workflows involved to build the solution, exceptions in the workflows and any assumptions that have been considered. Once agreed as the basis for the building of the project, the flowchart and assumptions will be used as a platform from which the solution will be designed. Changes to this business process may constitute a request for change and will be subject to the agreed agility program change procedures. Note: All the code is written in python version 3.8.5 High Level Objectives The high-level objectives are: Enable reading/loading of data from the various sources and convert them into pandas dataframe(details mentioned in the Data Ingestion Section). Enable reading various file formats and convert them into pandas dataframe(details mentioned in the Data Ingestion Section). Give user the option to specify feature and target columns. Give user the option to select the problem type, viz. Classification. Perform statistical analytics of the data and prepare a table for the analysis and show it on screen. Perform graphical analysis for the data and Showcase the results (graphs) on the screen. Showcase the graphical analysis once again for comparison. Choose the appropriate ML model for training. Perform Model Tuning. Perform Model Calibration. Perform Model Evaluation. Perform Model Prediction. Docker container creation. Cloud deployment.","title":"Introduction"},{"location":"introduction/#introduction","text":"","title":"Introduction"},{"location":"introduction/#data-description","text":"Pulsar candidates collected during the HTRU survey. Pulsars are a type of star, of considerable scientific interest. Candidates must be classified in to pulsar and non-pulsar classes to aid discovery. For full documentation visit HTRU2 Data Set . The data set shared here contains 16,259 spurious examples caused by RFI/noise, and 1,639 real pulsar examples. These examples have all been checked by human annotators. So our goal is solving the classification problem and predict which stars are pulsar. The technical design document gives a design blueprint of the Pulsar Classifier project. This document communicates the technical details of the solution proposed. In addition, this document also captures the different workflows involved to build the solution, exceptions in the workflows and any assumptions that have been considered. Once agreed as the basis for the building of the project, the flowchart and assumptions will be used as a platform from which the solution will be designed. Changes to this business process may constitute a request for change and will be subject to the agreed agility program change procedures. Note: All the code is written in python version 3.8.5","title":"Data Description"},{"location":"introduction/#high-level-objectives","text":"The high-level objectives are: Enable reading/loading of data from the various sources and convert them into pandas dataframe(details mentioned in the Data Ingestion Section). Enable reading various file formats and convert them into pandas dataframe(details mentioned in the Data Ingestion Section). Give user the option to specify feature and target columns. Give user the option to select the problem type, viz. Classification. Perform statistical analytics of the data and prepare a table for the analysis and show it on screen. Perform graphical analysis for the data and Showcase the results (graphs) on the screen. Showcase the graphical analysis once again for comparison. Choose the appropriate ML model for training. Perform Model Tuning. Perform Model Calibration. Perform Model Evaluation. Perform Model Prediction. Docker container creation. Cloud deployment.","title":"High Level Objectives"},{"location":"local/","text":"Deploy your model locally Navigate to project folder cd pulsar-classifier cortex deploy takes your model along with the configuration from cortex.yaml and creates a web API: cortex deploy Monitor the status of your API using cortex get: cortex get iris-classifier --watch status up-to-date requested last update avg request 2XX live 1 1 1m - - endpoint: http://localhost:8888 You can also stream logs from your API: cortex logs pulsar-classifier","title":"Deploy Locally"},{"location":"local/#deploy-your-model-locally","text":"Navigate to project folder cd pulsar-classifier cortex deploy takes your model along with the configuration from cortex.yaml and creates a web API: cortex deploy Monitor the status of your API using cortex get: cortex get iris-classifier --watch status up-to-date requested last update avg request 2XX live 1 1 1m - - endpoint: http://localhost:8888 You can also stream logs from your API: cortex logs pulsar-classifier","title":"Deploy your model locally"},{"location":"model/","text":"Model Selection Extra Trees Classifier Extra Trees Classifier Evaluation evaluate_model ( et ) Extra Trees Classifier Evaluation ROC Curve Confusion Matrix Calibration Plot Class Error PR Curve Class Report Tuned Extra Trees Classifier Evaluation evaluate_model ( tuned_et ) Tuned Extra Trees Classifier Evaluation ROC Curve Confusion Matrix PR Curve Class Error Class Report Calibration Plot Calibrated Extra Trees Classifier Evaluation evaluate_model ( calibrated_et ) Calibrated Extra Trees Classifier Evaluation ROC Curve Confusion Matrix PR Curve Class Error Class Report Calibration Plot Calibrated Tuned Extra Trees Classifier Evaluation evaluate_model ( calibrated_tuned_et ) Calibrated Tuned Extra Trees Classifier Evaluation ROC Curve Confusion Matrix PR Curve Class Error Class Report Calibration Plot Random Forest Classifier Random Forest Classifier Evaluation evaluate_model ( rf ) Random Forest Classifier Evaluation ROC Curve Confusion Matrix Calibration Plot Class Error PR Curve Class Report Tuned Random Forest Classifier Evaluation evaluate_model ( tuned_rf ) Tuned Random Forest Classifier Evaluation ROC Curve Confusion Matrix PR Curve Class Error Class Report Calibration Plot Calibrated Random Forest Classifier Evaluation evaluate_model ( calibrated_rf ) Calibrated Random Forest Classifier Evaluation ROC Curve Confusion Matrix PR Curve Class Error Class Report Calibration Plot Calibrated Tuned Random Forest Classifier Evaluation evaluate_model ( calibrated_tuned_rf ) Calibrated Tuned Random Forest Classifier Evaluation ROC Curve Confusion Matrix PR Curve Class Error Class Report Calibration Plot Classification Metrics Accuracy Accuracy simply measures how often the classifier makes the correct prediction. It\u2019s the ratio between the number of correct predictions and the total number of predictions (the number of test data points). Accuracy looks easy enough. However, it makes no distinction between classes; correct answers for each class are treated equally. Sometimes this is not enough. You might want to look at how many examples failed for each class. This would be the case if the cost of misclassification is different, or if you have a lot more test data of one class than the other. For instance, making the call that a patient has cancer when he doesn\u2019t (known as a false positive) has very different consequences than making the call that a patient doesn\u2019t have cancer when he does (a false negative). Precision & Recall Precision and recall are actually two metrics. But they are often used together. Precision answers the question: Out of the items that the classifier predicted to be true, how many are actually true? Whereas, recall answers the question: Out of all the items that are true, how many are found to be true by the classifier? The precision score quantifies the ability of a classifier to not label a negative example as positive. The precision score can be interpreted as the probability that a positive prediction made by the classifier is positive. The score is in the range [0,1] with 0 being the worst, and 1 being perfect. The precision and recall scores can be defined as: F-scores (F1, F-beta) The F1-score is a single metric that combines both precision and recall via their harmonic mean: The score lies in the range [0,1] with 1 being ideal and 0 being the worst. Unlike the arithmetic mean, the harmonic mean tends toward the smaller of the two elements. Hence the F1 score will be small if either precision or recall is small. The F1-score (sometimes known as the balanced F-beta score), is a special case of a metric known as the F-Beta score, which measures the effectiveness of retrieval with respect to a user who attaches \u03b2 times as much importance to recall as to precision. AUC: Area Under the ROC Curve AUC stands for \"Area under the ROC Curve.\" That is, AUC measures the entire two-dimensional area underneath the entire ROC curve (think integral calculus) from (0,0) to (1,1). The score lies in the range [0,1] with 1 being ideal and 0 being the worst. Unlike the arithmetic mean, the harmonic mean tends toward the smaller of the two elements. Hence the F1 score will be small if either precision or recall is small. The F1-score (sometimes known as the balanced F-beta score), is a special case of a metric known as the F-Beta score, which measures the effectiveness of retrieval with respect to a user who attaches \u03b2 times as much importance to recall as to precision. Sensitivity / True Positive Rate / Recall Sensitivity tells us what proportion of the positive class got correctly classified. A simple example would be to determine what proportion of the actual sick people were correctly detected by the model. False Negative Rate False Negative Rate (FNR) tells us what proportion of the positive class got incorrectly classified by the classifier. A higher TPR and a lower FNR is desirable since we want to correctly classify the positive class. Specificity / True Negative Rate Specificity tells us what proportion of the negative class got correctly classified. Taking the same example as in Sensitivity, Specificity would mean determining the proportion of healthy people who were correctly identified by the model. False Positive Rate FPR tells us what proportion of the negative class got incorrectly classified by the classifier. A higher TNR and a lower FPR is desirable since we want to correctly classify the negative class. Out of these metrics, Sensitivity and Specificity are perhaps the most important and we will see later on how these are used to build an evaluation metric. But before that, let\u2019s understand why the probability of prediction is better than predicting the target class directly. Confusion Matrix Positive Prediction Negative Prediction Positive Class True Positive (TP) False Negative (FN) Negative Class False Positive (FP) True Negative (TN)","title":"Model Evaluation"},{"location":"model/#model-selection","text":"","title":"Model Selection"},{"location":"model/#extra-trees-classifier","text":"","title":"Extra Trees Classifier"},{"location":"model/#extra-trees-classifier-evaluation","text":"evaluate_model ( et ) Extra Trees Classifier Evaluation ROC Curve Confusion Matrix Calibration Plot Class Error PR Curve Class Report","title":"Extra Trees Classifier Evaluation"},{"location":"model/#tuned-extra-trees-classifier-evaluation","text":"evaluate_model ( tuned_et ) Tuned Extra Trees Classifier Evaluation ROC Curve Confusion Matrix PR Curve Class Error Class Report Calibration Plot","title":"Tuned Extra Trees Classifier Evaluation"},{"location":"model/#calibrated-extra-trees-classifier-evaluation","text":"evaluate_model ( calibrated_et ) Calibrated Extra Trees Classifier Evaluation ROC Curve Confusion Matrix PR Curve Class Error Class Report Calibration Plot","title":"Calibrated Extra Trees Classifier Evaluation"},{"location":"model/#calibrated-tuned-extra-trees-classifier-evaluation","text":"evaluate_model ( calibrated_tuned_et ) Calibrated Tuned Extra Trees Classifier Evaluation ROC Curve Confusion Matrix PR Curve Class Error Class Report Calibration Plot","title":"Calibrated Tuned Extra Trees Classifier Evaluation"},{"location":"model/#random-forest-classifier","text":"","title":"Random Forest Classifier"},{"location":"model/#random-forest-classifier-evaluation","text":"evaluate_model ( rf ) Random Forest Classifier Evaluation ROC Curve Confusion Matrix Calibration Plot Class Error PR Curve Class Report","title":"Random Forest Classifier Evaluation"},{"location":"model/#tuned-random-forest-classifier-evaluation","text":"evaluate_model ( tuned_rf ) Tuned Random Forest Classifier Evaluation ROC Curve Confusion Matrix PR Curve Class Error Class Report Calibration Plot","title":"Tuned Random Forest Classifier Evaluation"},{"location":"model/#calibrated-random-forest-classifier-evaluation","text":"evaluate_model ( calibrated_rf ) Calibrated Random Forest Classifier Evaluation ROC Curve Confusion Matrix PR Curve Class Error Class Report Calibration Plot","title":"Calibrated Random Forest Classifier Evaluation"},{"location":"model/#calibrated-tuned-random-forest-classifier-evaluation","text":"evaluate_model ( calibrated_tuned_rf ) Calibrated Tuned Random Forest Classifier Evaluation ROC Curve Confusion Matrix PR Curve Class Error Class Report Calibration Plot","title":"Calibrated Tuned Random Forest Classifier Evaluation"},{"location":"model/#classification-metrics","text":"Accuracy Accuracy simply measures how often the classifier makes the correct prediction. It\u2019s the ratio between the number of correct predictions and the total number of predictions (the number of test data points). Accuracy looks easy enough. However, it makes no distinction between classes; correct answers for each class are treated equally. Sometimes this is not enough. You might want to look at how many examples failed for each class. This would be the case if the cost of misclassification is different, or if you have a lot more test data of one class than the other. For instance, making the call that a patient has cancer when he doesn\u2019t (known as a false positive) has very different consequences than making the call that a patient doesn\u2019t have cancer when he does (a false negative). Precision & Recall Precision and recall are actually two metrics. But they are often used together. Precision answers the question: Out of the items that the classifier predicted to be true, how many are actually true? Whereas, recall answers the question: Out of all the items that are true, how many are found to be true by the classifier? The precision score quantifies the ability of a classifier to not label a negative example as positive. The precision score can be interpreted as the probability that a positive prediction made by the classifier is positive. The score is in the range [0,1] with 0 being the worst, and 1 being perfect. The precision and recall scores can be defined as: F-scores (F1, F-beta) The F1-score is a single metric that combines both precision and recall via their harmonic mean: The score lies in the range [0,1] with 1 being ideal and 0 being the worst. Unlike the arithmetic mean, the harmonic mean tends toward the smaller of the two elements. Hence the F1 score will be small if either precision or recall is small. The F1-score (sometimes known as the balanced F-beta score), is a special case of a metric known as the F-Beta score, which measures the effectiveness of retrieval with respect to a user who attaches \u03b2 times as much importance to recall as to precision. AUC: Area Under the ROC Curve AUC stands for \"Area under the ROC Curve.\" That is, AUC measures the entire two-dimensional area underneath the entire ROC curve (think integral calculus) from (0,0) to (1,1). The score lies in the range [0,1] with 1 being ideal and 0 being the worst. Unlike the arithmetic mean, the harmonic mean tends toward the smaller of the two elements. Hence the F1 score will be small if either precision or recall is small. The F1-score (sometimes known as the balanced F-beta score), is a special case of a metric known as the F-Beta score, which measures the effectiveness of retrieval with respect to a user who attaches \u03b2 times as much importance to recall as to precision. Sensitivity / True Positive Rate / Recall Sensitivity tells us what proportion of the positive class got correctly classified. A simple example would be to determine what proportion of the actual sick people were correctly detected by the model. False Negative Rate False Negative Rate (FNR) tells us what proportion of the positive class got incorrectly classified by the classifier. A higher TPR and a lower FNR is desirable since we want to correctly classify the positive class. Specificity / True Negative Rate Specificity tells us what proportion of the negative class got correctly classified. Taking the same example as in Sensitivity, Specificity would mean determining the proportion of healthy people who were correctly identified by the model. False Positive Rate FPR tells us what proportion of the negative class got incorrectly classified by the classifier. A higher TNR and a lower FPR is desirable since we want to correctly classify the negative class. Out of these metrics, Sensitivity and Specificity are perhaps the most important and we will see later on how these are used to build an evaluation metric. But before that, let\u2019s understand why the probability of prediction is better than predicting the target class directly. Confusion Matrix Positive Prediction Negative Prediction Positive Class True Positive (TP) False Negative (FN) Negative Class False Positive (FP) True Negative (TN)","title":"Classification Metrics"},{"location":"pre/","text":"Prerequisite Create & Configure AWS EC2 Server For instructions follow below video till putty sign in https://youtu.be/ZHjOF4XBdiw After logging into your EC2 Instances using Putty Install AWS CLI V2 Make sure you've root access sudo su Now install AWS CLI V2 curl \"https://awscli.amazonaws.com/awscli-exe-linux-x86_64.zip\" -o \"awscliv2.zip\" unzip awscliv2.zip sudo ./aws/install Verify aws cli aws --version Configure AWS CLI $ aws configure AWS Access Key ID [None]: AKIAIOSFODNN7EXAMPLE AWS Secret Access Key [None]: wJalrXUtnFEMI/K7MDENG/bPxRfiCYEXAMPLEKEY Default region name [None]: us-west-2 Default output format [None]: json For more information on AWS CLI After configuring you aws server Install Docker curl -fsSL https://get.docker.com -o get-docker.sh sudo sh get-docker.sh Verify docker docker --version Install Cortex MLOps You must have Docker installed to run Cortex locally. bash -c \"$(curl -sS https://raw.githubusercontent.com/cortexlabs/cortex/0.18/get-cli.sh)\" verify cortex cortex --version","title":"Prerequisite"},{"location":"pre/#prerequisite","text":"","title":"Prerequisite"},{"location":"pre/#create-configure-aws-ec2-server","text":"For instructions follow below video till putty sign in https://youtu.be/ZHjOF4XBdiw After logging into your EC2 Instances using Putty","title":"Create &amp; Configure AWS EC2 Server"},{"location":"pre/#install-aws-cli-v2","text":"Make sure you've root access sudo su Now install AWS CLI V2 curl \"https://awscli.amazonaws.com/awscli-exe-linux-x86_64.zip\" -o \"awscliv2.zip\" unzip awscliv2.zip sudo ./aws/install Verify aws cli aws --version Configure AWS CLI $ aws configure AWS Access Key ID [None]: AKIAIOSFODNN7EXAMPLE AWS Secret Access Key [None]: wJalrXUtnFEMI/K7MDENG/bPxRfiCYEXAMPLEKEY Default region name [None]: us-west-2 Default output format [None]: json For more information on AWS CLI After configuring you aws server","title":"Install AWS CLI V2"},{"location":"pre/#install-docker","text":"curl -fsSL https://get.docker.com -o get-docker.sh sudo sh get-docker.sh Verify docker docker --version","title":"Install Docker"},{"location":"pre/#install-cortex-mlops","text":"You must have Docker installed to run Cortex locally. bash -c \"$(curl -sS https://raw.githubusercontent.com/cortexlabs/cortex/0.18/get-cli.sh)\" verify cortex cortex --version","title":"Install Cortex MLOps"},{"location":"predict/","text":"Model Prediction Predict Extra Trees Classifier pred_et = predict_model ( et ) pred_et_1 = predict_model ( calibrated_et ) pred_et_2 = predict_model ( tuned_et ) pred_et_3 = predict_model ( calibrated_tuned_et ) Predicted Values Matrix Accuracy AUC F1 et 0.9819 0.9729 0.8967 calibrated_et 0.9818 0.9783 0.8951 tuned_et 0.9818 0.9681 0.8955 calibrated_tuned_et 0.9816 0.9779 0.8941 Predict Random Forest Classifier pred_rf = predict_model ( rf ) pred_rf_1 = predict_model ( calibrated_rf ) pred_rf_2 = predict_model ( tuned_rf ) pred_rf_3 = predict_model ( calibrated_tuned_rf ) Predicted Values Matrix Accuracy AUC F1 rf 0.9803 0.9612 0.8870 calibrated_rf 0.9818 0.9743 0.8946 tuned_rf 0.9806 0.9753 0.8905 calibrated_tuned_rf 0.9799 0.9758 0.8851","title":"Model Prediction"},{"location":"predict/#model-prediction","text":"","title":"Model Prediction"},{"location":"predict/#predict-extra-trees-classifier","text":"pred_et = predict_model ( et ) pred_et_1 = predict_model ( calibrated_et ) pred_et_2 = predict_model ( tuned_et ) pred_et_3 = predict_model ( calibrated_tuned_et ) Predicted Values Matrix Accuracy AUC F1 et 0.9819 0.9729 0.8967 calibrated_et 0.9818 0.9783 0.8951 tuned_et 0.9818 0.9681 0.8955 calibrated_tuned_et 0.9816 0.9779 0.8941","title":"Predict Extra Trees Classifier"},{"location":"predict/#predict-random-forest-classifier","text":"pred_rf = predict_model ( rf ) pred_rf_1 = predict_model ( calibrated_rf ) pred_rf_2 = predict_model ( tuned_rf ) pred_rf_3 = predict_model ( calibrated_tuned_rf ) Predicted Values Matrix Accuracy AUC F1 rf 0.9803 0.9612 0.8870 calibrated_rf 0.9818 0.9743 0.8946 tuned_rf 0.9806 0.9753 0.8905 calibrated_tuned_rf 0.9799 0.9758 0.8851","title":"Predict Random Forest Classifier"},{"location":"preprocessing/","text":"Data Preprocessing Install Pycaret pip install pycaret == 2.0 Importing Classification Module from pycaret.classification import * Setup Preprocessing Pipeline Synthetic Minority Oversampling Technique An improvement on duplicating examples from the minority class is to synthesize new examples from the minority class. This is a type of data augmentation for tabular data and can be very effective. SMOTE works by selecting examples that are close in the feature space, drawing a line between the examples in the feature space and drawing a new sample at a point along that line. SMOTE first selects a minority class instance a at random and finds its k nearest minority class neighbors. The synthetic instance is then created by choosing one of the k nearest neighbors b at random and connecting a and b to form a line segment in the feature space. The synthetic instances are generated as a convex combination of the two chosen instances a and b. Feature Selection Feature Importance is a process used to select features in the dataset that contributes the most in predicting the target variable. Working with selected features instead of all the features reduces the risk of over-fitting, improves accuracy, and decreases the training time.It uses a combination of several supervised feature selection techniques to select the subset of features that are most important for modeling. A subset of features are selected using a combination of various permutation importance techniques including Random Forest, Adaboost and Linear correlation with target variable. setup ( data = df , target = 'class' , feature_selection = True , fix_imbalance = True , log_experiment = True , experiment_name = 'Feature Selection/Importance & SMOTE' ) Output Comparing Models Models Considered ada - Adaboost gbc - Gradient Boosting Classifier xgboost - Extreme Gradient Boosting lightgbm - Light Gradient Boosting rf - Random Forest dt - Decision Tree et - Extra Trees Classifier catboost compare_models ( whitelist = [ 'ada' , 'gbc' , 'xgboost' , 'lightgbm' , 'rf' , 'dt' , 'et' , 'catboost' ]) Output","title":"Data Preprocessing"},{"location":"preprocessing/#data-preprocessing","text":"","title":"Data Preprocessing"},{"location":"preprocessing/#install-pycaret","text":"pip install pycaret == 2.0","title":"Install Pycaret"},{"location":"preprocessing/#importing-classification-module","text":"from pycaret.classification import *","title":"Importing Classification Module"},{"location":"preprocessing/#setup-preprocessing-pipeline","text":"Synthetic Minority Oversampling Technique An improvement on duplicating examples from the minority class is to synthesize new examples from the minority class. This is a type of data augmentation for tabular data and can be very effective. SMOTE works by selecting examples that are close in the feature space, drawing a line between the examples in the feature space and drawing a new sample at a point along that line. SMOTE first selects a minority class instance a at random and finds its k nearest minority class neighbors. The synthetic instance is then created by choosing one of the k nearest neighbors b at random and connecting a and b to form a line segment in the feature space. The synthetic instances are generated as a convex combination of the two chosen instances a and b. Feature Selection Feature Importance is a process used to select features in the dataset that contributes the most in predicting the target variable. Working with selected features instead of all the features reduces the risk of over-fitting, improves accuracy, and decreases the training time.It uses a combination of several supervised feature selection techniques to select the subset of features that are most important for modeling. A subset of features are selected using a combination of various permutation importance techniques including Random Forest, Adaboost and Linear correlation with target variable. setup ( data = df , target = 'class' , feature_selection = True , fix_imbalance = True , log_experiment = True , experiment_name = 'Feature Selection/Importance & SMOTE' ) Output","title":"Setup Preprocessing Pipeline"},{"location":"preprocessing/#comparing-models","text":"","title":"Comparing Models"},{"location":"preprocessing/#models-considered","text":"ada - Adaboost gbc - Gradient Boosting Classifier xgboost - Extreme Gradient Boosting lightgbm - Light Gradient Boosting rf - Random Forest dt - Decision Tree et - Extra Trees Classifier catboost compare_models ( whitelist = [ 'ada' , 'gbc' , 'xgboost' , 'lightgbm' , 'rf' , 'dt' , 'et' , 'catboost' ]) Output","title":"Models Considered"},{"location":"profile/","text":"Data Profiling Pandas Profiling Install pip install - U pandas - profiling [ notebook ] jupyter nbextension enable -- py widgetsnbextension You may have to restart the kernel or runtime. Configure from pandas_profiling import ProfileReport profile = ProfileReport ( df , title = 'Data Profiling' , explorative = True ) profile . to_file ( \"data_profile.html\" ) Click here to see Data Profiling Report","title":"Data Profiling"},{"location":"profile/#data-profiling","text":"","title":"Data Profiling"},{"location":"profile/#pandas-profiling","text":"","title":"Pandas Profiling"},{"location":"profile/#install","text":"pip install - U pandas - profiling [ notebook ] jupyter nbextension enable -- py widgetsnbextension You may have to restart the kernel or runtime.","title":"Install"},{"location":"profile/#configure","text":"from pandas_profiling import ProfileReport profile = ProfileReport ( df , title = 'Data Profiling' , explorative = True ) profile . to_file ( \"data_profile.html\" ) Click here to see Data Profiling Report","title":"Configure"},{"location":"raw/","text":"Main Heading Some Descriptions. For full documentation visit mkdocs.org . Commands mkdocs new [dir-name] - Create a new project. mkdocs serve - Start the live-reloading docs server. mkdocs build - Build the documentation site. mkdocs -h - Print help message and exit. Project layout mkdocs.yml # The configuration file. docs/ index.md # The documentation homepage. ... # Other markdown pages, images and other files. Code # Create a dataset. dataset = keras . preprocessing . image_dataset_from_directory ( 'path/to/main_directory' , batch_size = 64 , image_size = ( 200 , 200 )) # For demonstration, iterate over the batches yielded by the dataset. for data , labels in dataset : print ( data . shape ) # (64, 200, 200, 3) print ( data . dtype ) # float32 print ( labels . shape ) # (64,) print ( labels . dtype ) # int32 Nulla et rhoncus turpis. Mauris ultricies elementum leo. Duis efficitur accumsan nibh eu mattis. Vivamus tempus velit eros, porttitor placerat nibh lacinia sed. Aenean in finibus diam. Duis mollis est eget nibh volutpat, fermentum aliquet dui mollis. Nam vulputate tincidunt fringilla. Nullam dignissim ultrices urna non auctor. Lorem ipsum dolor sit amet : Sed sagittis eleifend rutrum. Donec vitae suscipit est. Nullam tempus tellus non sem sollicitudin, quis rutrum leo facilisis. Table Method Description GET Fetch resource PUT Update resource DELETE Delete resource Phasellus posuere in sem ut cursus Lorem ipsum dolor sit amet, consectetur adipiscing elit. Nulla et euismod nulla. Curabitur feugiat, tortor non consequat finibus, justo purus auctor massa, nec semper lorem quam in massa. Phasellus posuere in sem ut cursus Lorem ipsum dolor sit amet, consectetur adipiscing elit. Nulla et euismod nulla. Curabitur feugiat, tortor non consequat finibus, justo purus auctor massa, nec semper lorem quam in massa. def bubble_sort ( items ): for i in range ( len ( items )): for j in range ( len ( items ) - 1 - i ): if items [ j ] > items [ j + 1 ]: items [ j ], items [ j + 1 ] = items [ j + 1 ], items [ j ] Nunc eu odio eleifend, blandit leo a, volutpat sapien. Phasellus posuere in sem ut cursus. Nullam sit amet tincidunt ipsum, sit amet elementum turpis. Etiam ipsum quam, mattis in purus vitae, lacinia fermentum enim. Phasellus posuere in sem ut cursus Lorem ipsum dolor sit amet, consectetur adipiscing elit. Nulla et euismod nulla. Curabitur feugiat, tortor non consequat finibus, justo purus auctor massa, nec semper lorem quam in massa. Phasellus posuere in sem ut cursus Lorem ipsum dolor sit amet, consectetur adipiscing elit. Nulla et euismod nulla. Curabitur feugiat, tortor non consequat finibus, justo purus auctor massa, nec semper lorem quam in massa.","title":"Main Heading"},{"location":"raw/#main-heading","text":"Some Descriptions. For full documentation visit mkdocs.org .","title":"Main Heading"},{"location":"raw/#commands","text":"mkdocs new [dir-name] - Create a new project. mkdocs serve - Start the live-reloading docs server. mkdocs build - Build the documentation site. mkdocs -h - Print help message and exit.","title":"Commands"},{"location":"raw/#project-layout","text":"mkdocs.yml # The configuration file. docs/ index.md # The documentation homepage. ... # Other markdown pages, images and other files.","title":"Project layout"},{"location":"raw/#code","text":"# Create a dataset. dataset = keras . preprocessing . image_dataset_from_directory ( 'path/to/main_directory' , batch_size = 64 , image_size = ( 200 , 200 )) # For demonstration, iterate over the batches yielded by the dataset. for data , labels in dataset : print ( data . shape ) # (64, 200, 200, 3) print ( data . dtype ) # float32 print ( labels . shape ) # (64,) print ( labels . dtype ) # int32 Nulla et rhoncus turpis. Mauris ultricies elementum leo. Duis efficitur accumsan nibh eu mattis. Vivamus tempus velit eros, porttitor placerat nibh lacinia sed. Aenean in finibus diam. Duis mollis est eget nibh volutpat, fermentum aliquet dui mollis. Nam vulputate tincidunt fringilla. Nullam dignissim ultrices urna non auctor. Lorem ipsum dolor sit amet : Sed sagittis eleifend rutrum. Donec vitae suscipit est. Nullam tempus tellus non sem sollicitudin, quis rutrum leo facilisis.","title":"Code"},{"location":"raw/#table","text":"Method Description GET Fetch resource PUT Update resource DELETE Delete resource Phasellus posuere in sem ut cursus Lorem ipsum dolor sit amet, consectetur adipiscing elit. Nulla et euismod nulla. Curabitur feugiat, tortor non consequat finibus, justo purus auctor massa, nec semper lorem quam in massa. Phasellus posuere in sem ut cursus Lorem ipsum dolor sit amet, consectetur adipiscing elit. Nulla et euismod nulla. Curabitur feugiat, tortor non consequat finibus, justo purus auctor massa, nec semper lorem quam in massa. def bubble_sort ( items ): for i in range ( len ( items )): for j in range ( len ( items ) - 1 - i ): if items [ j ] > items [ j + 1 ]: items [ j ], items [ j + 1 ] = items [ j + 1 ], items [ j ] Nunc eu odio eleifend, blandit leo a, volutpat sapien. Phasellus posuere in sem ut cursus. Nullam sit amet tincidunt ipsum, sit amet elementum turpis. Etiam ipsum quam, mattis in purus vitae, lacinia fermentum enim. Phasellus posuere in sem ut cursus Lorem ipsum dolor sit amet, consectetur adipiscing elit. Nulla et euismod nulla. Curabitur feugiat, tortor non consequat finibus, justo purus auctor massa, nec semper lorem quam in massa. Phasellus posuere in sem ut cursus Lorem ipsum dolor sit amet, consectetur adipiscing elit. Nulla et euismod nulla. Curabitur feugiat, tortor non consequat finibus, justo purus auctor massa, nec semper lorem quam in massa.","title":"Table"},{"location":"save/","text":"Save Model Save Model in Pickle Format save_model ( final_rf , 'model' ) Install Boto3 pip install sklearn boto3 Upload Pickle Model to S3 Bucket Run the script locally import boto3 import pickle s3 = boto3 . client ( \"s3\" ) s3 . upload_file ( \"model.pkl\" , \"pulsar\" , \"pulsar-classifier/model.pkl\" ) Model Path: s3//pulsar/pulsar-classifier/model.pkl Save Model in ONNX Format What is ONNX ONNX provides a definition of an extensible computation graph model, as well as definitions of built-in operators and standard data types. Each computation dataflow graph is structured as a list of nodes that form an acyclic graph. Nodes have one or more inputs and one or more outputs. Each node is a call to an operator. The graph also has metadata to help document its purpose, author, etc. Operators are implemented externally to the graph, but the set of built-in operators are portable across frameworks. Every framework supporting ONNX will provide implementations of these operators on the applicable data types. Install skl2onnx pip install skl2onnx Convert into ONNX format with onnxmltools from skl2onnx import convert_sklearn from skl2onnx.common.data_types import FloatTensorType initial_type = [( 'float_input' , FloatTensorType ([ None , 8 ]))] onx = convert_sklearn ( final_rf , initial_types = initial_type ) with open ( \"model.onnx\" , \"wb\" ) as f : f . write ( onx . SerializeToString ()) Upload ONNX Model to S3 Bucket Run the script locally import boto3 import pickle s3 = boto3 . client ( \"s3\" ) s3 . upload_file ( \"model.onnx\" , \"pulsar\" , \"pulsar-classifier/model.onnx\" ) Model Path: s3//pulsar/pulsar-classifier/model.onnx Manually upload model to S3 Bucket Caution You need to write aws credential files and config files with latest model path. Follow official S3 documentation. Follow below tutorial to make your model available public on S3 https://www.youtube.com/watch?v=s1Tu0yKmDKU OR https://www.youtube.com/watch?v=ZmLrf_BBxIc&t=506s","title":"Save Model"},{"location":"save/#save-model","text":"","title":"Save Model"},{"location":"save/#save-model-in-pickle-format","text":"save_model ( final_rf , 'model' )","title":"Save Model in Pickle Format"},{"location":"save/#install-boto3","text":"pip install sklearn boto3","title":"Install Boto3"},{"location":"save/#upload-pickle-model-to-s3-bucket","text":"Run the script locally import boto3 import pickle s3 = boto3 . client ( \"s3\" ) s3 . upload_file ( \"model.pkl\" , \"pulsar\" , \"pulsar-classifier/model.pkl\" ) Model Path: s3//pulsar/pulsar-classifier/model.pkl","title":"Upload Pickle Model to S3 Bucket"},{"location":"save/#save-model-in-onnx-format","text":"What is ONNX ONNX provides a definition of an extensible computation graph model, as well as definitions of built-in operators and standard data types. Each computation dataflow graph is structured as a list of nodes that form an acyclic graph. Nodes have one or more inputs and one or more outputs. Each node is a call to an operator. The graph also has metadata to help document its purpose, author, etc. Operators are implemented externally to the graph, but the set of built-in operators are portable across frameworks. Every framework supporting ONNX will provide implementations of these operators on the applicable data types.","title":"Save Model in ONNX Format"},{"location":"save/#install-skl2onnx","text":"pip install skl2onnx","title":"Install skl2onnx"},{"location":"save/#convert-into-onnx-format-with-onnxmltools","text":"from skl2onnx import convert_sklearn from skl2onnx.common.data_types import FloatTensorType initial_type = [( 'float_input' , FloatTensorType ([ None , 8 ]))] onx = convert_sklearn ( final_rf , initial_types = initial_type ) with open ( \"model.onnx\" , \"wb\" ) as f : f . write ( onx . SerializeToString ())","title":"Convert into ONNX format with onnxmltools"},{"location":"save/#upload-onnx-model-to-s3-bucket","text":"Run the script locally import boto3 import pickle s3 = boto3 . client ( \"s3\" ) s3 . upload_file ( \"model.onnx\" , \"pulsar\" , \"pulsar-classifier/model.onnx\" ) Model Path: s3//pulsar/pulsar-classifier/model.onnx","title":"Upload ONNX Model to S3 Bucket"},{"location":"save/#manually-upload-model-to-s3-bucket","text":"Caution You need to write aws credential files and config files with latest model path. Follow official S3 documentation. Follow below tutorial to make your model available public on S3 https://www.youtube.com/watch?v=s1Tu0yKmDKU OR https://www.youtube.com/watch?v=ZmLrf_BBxIc&t=506s","title":"Manually upload model to S3 Bucket"},{"location":"tune/","text":"Model Tuning Model Tuning It tunes the hyperparameter of the model passed as an estimator using Random grid search with pre-defined grids that are fully customizable. Tune Extra Trees Classifier # optimize = \"Accuracy\", choose_better = False # AUC, F1, Precision, Recall, Accuracy tuned_et = tune_model ( et ) Output Tune Random Forest Classifier # optimize = \"Accuracy\", choose_better = False # AUC, F1, Precision, Recall, Accuracy tuned_rf = tune_model ( rf , optimize = \"Recall\" ) Output","title":"Model Tuning"},{"location":"tune/#model-tuning","text":"Model Tuning It tunes the hyperparameter of the model passed as an estimator using Random grid search with pre-defined grids that are fully customizable.","title":"Model Tuning"},{"location":"tune/#tune-extra-trees-classifier","text":"# optimize = \"Accuracy\", choose_better = False # AUC, F1, Precision, Recall, Accuracy tuned_et = tune_model ( et ) Output","title":"Tune Extra Trees Classifier"},{"location":"tune/#tune-random-forest-classifier","text":"# optimize = \"Accuracy\", choose_better = False # AUC, F1, Precision, Recall, Accuracy tuned_rf = tune_model ( rf , optimize = \"Recall\" ) Output","title":"Tune Random Forest Classifier"},{"location":"ui/","text":"User Interface UI Flowchart Clone Git Repository \u2013 https://github.com/erepr/pulsar-classifier git clone https//www.github.com/erepr/pulsar-classifier.git Install XAMPP Server https://www.apachefriends.org/index.html Add HTML Files copy and paste html folder to xampp\\htdocs directory Run XAMPP Server Enable Cross Origin Request To bypass CORS restriction install this plugin. This plugin allows you to send cross-domain requests. This plugin allows you to send cross-domain requests directly from browser without receiving Cross Origin Errors. Moesif Origin & CORS Changer Moesif Origin & CORS Changer .","title":"User Interface"},{"location":"ui/#user-interface","text":"","title":"User Interface"},{"location":"ui/#ui-flowchart","text":"","title":"UI Flowchart"},{"location":"ui/#clone-git-repository","text":"\u2013 https://github.com/erepr/pulsar-classifier git clone https//www.github.com/erepr/pulsar-classifier.git","title":"Clone Git Repository"},{"location":"ui/#install-xampp-server","text":"https://www.apachefriends.org/index.html","title":"Install XAMPP Server"},{"location":"ui/#add-html-files","text":"copy and paste html folder to xampp\\htdocs directory","title":"Add HTML Files"},{"location":"ui/#run-xampp-server","text":"Enable Cross Origin Request To bypass CORS restriction install this plugin. This plugin allows you to send cross-domain requests. This plugin allows you to send cross-domain requests directly from browser without receiving Cross Origin Errors. Moesif Origin & CORS Changer Moesif Origin & CORS Changer .","title":"Run XAMPP Server"},{"location":"workflow/","text":"Workflow Workflow Chart","title":"Workflow"},{"location":"workflow/#workflow","text":"","title":"Workflow"},{"location":"workflow/#workflow-chart","text":"","title":"Workflow Chart"}]}